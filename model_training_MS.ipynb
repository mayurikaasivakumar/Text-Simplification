{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayurikaasivakumar/Text-Simplification/blob/main/model_training_MS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngM_pkuatte1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ7Zxqq9ggwc"
      },
      "outputs": [],
      "source": [
        "%cd gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOLq8LPlusCE"
      },
      "outputs": [],
      "source": [
        "list_files = []\n",
        "for file in os.listdir():\n",
        "  if file.endswith('.csv'):\n",
        "\n",
        "  # you can change '.csv' to any other type of file, but take into account the pd.read_csv\n",
        "  # and change it to a pandas function that helps you read that type of file\n",
        "  # e.g pd.read_excel in case it is a .xlsx file\n",
        "  # or pd.read_json in case it is a .json file\n",
        "\n",
        "    df = pd.read_csv()\n",
        "\n",
        "    # you can add any piece of code inside this for loop in case\n",
        "    # you want to create a cleaning pipeline\n",
        "\n",
        "    list_files.append(df)\n",
        "\n",
        "df = pd.concat(list_files, axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80nHGOUnvDPi"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"18sat_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvbXwSLhvOR2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# libraries for the files in google drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import drive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yung2S4gjTe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import h5py\n",
        "import pathlib\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.utils import shuffle, class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import itertools\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, LSTM, GRU, RNN, CuDNNGRU, CuDNNLSTM, Bidirectional\n",
        "\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.core.debugger import set_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywPp6qaggtMT"
      },
      "outputs": [],
      "source": [
        "## global variable\n",
        "datanorm = True\n",
        "datacols = ['CURRENT_FIX_X', 'CURRENT_FIX_Y', 'CURRENT_FIX_PUPIL', 'CURRENT_FIX_DURATION']\n",
        "\n",
        "delta = 10 # size of window based on the middle point\n",
        "step =  2*delta+1 #if you want no overlap: 2*delta+1 # size of step in window extraction\n",
        "\n",
        "datasplit = 'subject'\n",
        "\n",
        "labelcols = ['subj', 'book',\n",
        "            'acc_level', 'subj_acc_level',\n",
        "            'confidence', 'difficulty', 'familiarity', 'recognition',\n",
        "            'interest', 'pressured', 'sleepiness', 'sleephours',\n",
        "            'sex', 'native']\n",
        "\n",
        "pred_variable = 'subj_acc_level'\n",
        "\n",
        "modeltype = 'cnn'\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 1000\n",
        "\n",
        "# data path\n",
        "file_fixdata = '../data/18sat_fixfinal.csv'\n",
        "file_trialdata = '../data/18sat_trialfinal.csv'\n",
        "file_labels = '../data/18sat_labels.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD5coREi6JU_"
      },
      "outputs": [],
      "source": [
        "## score file load\n",
        "sc = pd.read_csv(file_labels)\n",
        "\n",
        "## select label columns\n",
        "sc = sc[labelcols]\n",
        "\n",
        "## cut/replace values\n",
        "sc['sex'] = sc['sex'].replace(['F', 'M'], [1,0])\n",
        "binarycols = ('recognition', 'sex', 'native')\n",
        "subsetcols = [c for c in labelcols if c not in binarycols]\n",
        "sc[subsetcols] = sc[subsetcols].replace([0,1,2,3], [0,0,1,1])\n",
        "\n",
        "## frequency table per column\n",
        "for column in sc:\n",
        "    print(sc[column].value_counts(sort=False, dropna=False), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeSwjeWyAwM8"
      },
      "source": [
        "## create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYeAm06DHwkI"
      },
      "outputs": [],
      "source": [
        "## preprocessing to window\n",
        "def group_windows(fixationRows):\n",
        "    windows = []\n",
        "    fixationRows.reset_index(inplace=True)\n",
        "    for n in range(delta, len(fixationRows)-delta, step):\n",
        "        window = fixationRows.loc[n-delta:n+delta,datacols].values.tolist()\n",
        "        windows.append(window)\n",
        "    return windows\n",
        "\n",
        "# original version\n",
        "    # nRow = fixationRows.shape[0]\n",
        "    # nCol = fixationRows.shape[1]\n",
        "    # windows = []\n",
        "\n",
        "    # fixNum = 0\n",
        "\n",
        "    # for index, row in fixationRows.iterrows():\n",
        "    #     if (index+1)%\n",
        "    #     if fixNum + delta <= nRow-1 and fixNum - delta >= 0:\n",
        "    #         deltaMin = fixNum - delta\n",
        "    #         deltaMax = fixNum + delta\n",
        "    #         window = []\n",
        "    #         for i in range(deltaMin, deltaMax+1):\n",
        "    #             x = fixationRows['CURRENT_FIX_X']\n",
        "    #             x = x.values[i]\n",
        "    #             y = fixationRows['CURRENT_FIX_Y'] # - yOffset\n",
        "    #             y = y.values[i]\n",
        "    #             d = fixationRows['CURRENT_FIX_DURATION']\n",
        "    #             d = d.values[i]\n",
        "    #             p = fixationRows['CURRENT_FIX_PUPIL']\n",
        "    #             p = p.values[i]\n",
        "    #             #r = fixationRows['CURRENT_FIX_INTEREST_AREA_LABEL']\n",
        "    #             #r = r.values[i]\n",
        "    #             window.append([x, y, d, p])\n",
        "    #         windows.append(window)\n",
        "    #     fixNum += 1\n",
        "    # return windows\n",
        "\n",
        "## Loop over all articles and subjects\n",
        "def generate_windata(fixation):\n",
        "    subjectPool = pd.unique(fixation['RECORDING_SESSION_LABEL'])\n",
        "    pagePool = pd.unique(fixation['page_name'])\n",
        "    windowData = {}\n",
        "    for subject in subjectPool:\n",
        "        subjectRows = fixation.loc[fixation['RECORDING_SESSION_LABEL'] == subject]\n",
        "        windowData[subject] = {}\n",
        "        print(\"\\rprocessing Subject: \" + subject, end='')\n",
        "        for page in pagePool:\n",
        "            # print (\"Subject: \" + subject + \", Page: \" + page)\n",
        "            pageRows = subjectRows.loc[subjectRows['page_name'] == page]\n",
        "            # visualize_article(article, subjectRows)\n",
        "            windows = group_windows(pageRows)\n",
        "            windowData[subject][page] = windows\n",
        "    print (\"\\nwindow data ready\")\n",
        "    return windowData\n",
        "\n",
        "\n",
        "## create dataset\n",
        "def create_dataset(windowData, sc):\n",
        "    dataset = []\n",
        "    index= []\n",
        "    labeldf = pd.DataFrame()\n",
        "\n",
        "    for subject in windowData:\n",
        "        for article in windowData[subject]:\n",
        "            windows = windowData[subject][article]\n",
        "            for window in windows:\n",
        "                dataset.append(window)\n",
        "                book = article.split('-')[1] # article = 'reading-dickens-1'\n",
        "                row = sc[(sc['subj'] == subject) & (sc['book'] == book)]\n",
        "                labeldf = pd.concat([labeldf, row])\n",
        "\n",
        "    print('dataset created')\n",
        "    return np.array(dataset), labeldf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPPrP5y8nNYx"
      },
      "outputs": [],
      "source": [
        "## fixation data load\n",
        "fd = pd.read_csv(file_fixdata) #encoding = \"ISO-8859-1\",\\\n",
        "\n",
        "## data split\n",
        "fd_rd = fd.loc[fd.type == 'reading']\n",
        "# fd_comp = fd.loc[(fd.type == 'question') & (fd.page <= 5)]\n",
        "# fd_mental = fd.loc[(fd.type == 'question') & (fd.page > 5)]\n",
        "\n",
        "## data normalization\n",
        "if datanorm:\n",
        "    # normalized_df=(df-df.mean())/df.std()\n",
        "    # normalized_df=(df-df.min())/(df.max()-df.min())\n",
        "    # fd_rd_mean = fd_rd.copy(deep=True)\n",
        "    # fd_rd_mean[cols]=(fd_rd[cols]-fd_rd[cols].mean())/fd_rd[cols].std()\n",
        "    fixData = fd_rd.copy(deep=True)\n",
        "    fixData[datacols] = (fd_rd[datacols]-fd_rd[datacols].min())/(fd_rd[datacols].max()-fd_rd[datacols].min())\n",
        "else:\n",
        "    fixData = fd_rd.copy(deep=True)\n",
        "\n",
        "fixData[datacols].describe()\n",
        "\n",
        "# from sklearn import preprocessing\n",
        "\n",
        "# x = df.values #returns a numpy array\n",
        "# min_max_scaler = preprocessing.MinMaxScaler()\n",
        "# x_scaled = min_max_scaler.fit_transform(x)\n",
        "# df_scaled = pd.DataFrame(x_scaled)\n",
        "\n",
        "## call function\n",
        "windowData = generate_windata(fixData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pvcbHWns6qX"
      },
      "outputs": [],
      "source": [
        "## save windowdata\n",
        "with open('../data/windowData_'+str(delta)+'.pkl', 'wb') as fp:\n",
        "    pickle.dump(windowData, fp)\n",
        "\n",
        "# ## load windowdata\n",
        "# with open('../data/windowData_'+str(delta)+'.pkl', 'rb') as fp:\n",
        "#     windowData = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4YRDwEnisBYz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a8ada87a-f3b2-492c-9e0c-44c8e2e1a071"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1bf0f666cc28>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m## current plan is to use 50:25:25 (2,1,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0msubjkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindowData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mpagekeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindowData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubjkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mbookkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ['dickens' 'flytrap' 'genome' 'northpole']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'windowData' is not defined"
          ]
        }
      ],
      "source": [
        "## data split\n",
        "datasplit = 'book'\n",
        "\n",
        "if datasplit == 'subject':\n",
        "    # subject-wise dataset split  (subject wise)\n",
        "    # current plan is to use 60:20:20\n",
        "    subjkeys = list(windowData.keys())\n",
        "    random.Random(23).shuffle(subjkeys) #random shuffling\n",
        "    N_totalsub = len(subjkeys)\n",
        "    N_trainsub = round(0.6*N_totalsub)\n",
        "    N_validsub = round(0.2*N_totalsub)\n",
        "    N_testsub = N_totalsub - N_trainsub - N_validsub\n",
        "\n",
        "    windowData_train = deepcopy(windowData)\n",
        "    windowData_valid = {}\n",
        "    windowData_test = {}\n",
        "\n",
        "    for i, subj in enumerate(subjkeys):\n",
        "        if i in range(N_validsub):\n",
        "            #print(subj, 'to valid')\n",
        "            windowData_valid[subj] = windowData_train[subj]\n",
        "            del windowData_train[subj]\n",
        "        elif i in range(N_validsub, N_validsub + N_testsub):\n",
        "            #print(subj, 'to test')\n",
        "            windowData_test[subj] = windowData_train[subj]\n",
        "            del windowData_train[subj]\n",
        "\n",
        "    print(\"train subj #\", len(list(windowData_train.keys())))\n",
        "    print(\"valid subj #\", len(list(windowData_valid.keys())))\n",
        "    print(\"test subj #\", len(list(windowData_test.keys())))\n",
        "\n",
        "    ## create dataset\n",
        "    X_train, labels_train = create_dataset(windowData_train, sc)\n",
        "    X_valid, labels_valid = create_dataset(windowData_valid, sc)\n",
        "    X_test, labels_test = create_dataset(windowData_test, sc)\n",
        "\n",
        "elif datasplit == 'record':\n",
        "    X, labels = create_dataset(windowData, sc)\n",
        "    X_train, X_test, labels_train, labels_test = train_test_split(X, labels, test_size=0.4, random_state=23)\n",
        "    X_valid, X_test, labels_valid, labels_test = train_test_split(X_test, labels_test, test_size=0.5, random_state=23)\n",
        "\n",
        "elif datasplit == 'book':\n",
        "    # book-wise dataset split\n",
        "    ## current plan is to use 50:25:25 (2,1,1)\n",
        "\n",
        "    subjkeys = list(windowData.keys())\n",
        "    pagekeys = list(windowData[subjkeys[0]].keys())\n",
        "    bookkeys = list(np.unique(sc['book'])) # ['dickens' 'flytrap' 'genome' 'northpole']\n",
        "    print('list of books:', bookkeys)\n",
        "\n",
        "    windowData_train = deepcopy(windowData)\n",
        "    windowData_valid = defaultdict(dict)\n",
        "    windowData_test = defaultdict(dict)\n",
        "\n",
        "    for subj in subjkeys:\n",
        "        tmp = random.sample(bookkeys,2)\n",
        "        for page in pagekeys:\n",
        "            if (page.split('-')[1] == tmp[0]):\n",
        "                windowData_valid[subj][page] = windowData_train[subj][page]\n",
        "                del windowData_train[subj][page]\n",
        "\n",
        "            elif (page.split('-')[1] == tmp[1]):\n",
        "                windowData_test[subj][page] = windowData_train[subj][page]\n",
        "                del windowData_train[subj][page]\n",
        "\n",
        "    ## create dataset\n",
        "    X_train, labels_train = create_dataset(windowData_train, sc)\n",
        "    X_valid, labels_valid = create_dataset(windowData_valid, sc)\n",
        "    X_test, labels_test = create_dataset(windowData_test, sc)\n",
        "\n",
        "    print(\"train book #\", list(windowData_train['msd001'].keys()))\n",
        "    print(\"valid book #\", list(windowData_valid['msd001'].keys()))\n",
        "    print(\"test book #\", list(windowData_test['msd001'].keys()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFqqF4cgQnsE"
      },
      "outputs": [],
      "source": [
        "## save dataset\n",
        "np.save('../dataset/'+ datasplit +'wise/train/fix_train_' + str(delta) + '.npy', X_train)\n",
        "np.save('../dataset/' + datasplit + 'wise/val/fix_valid_' + str(delta) + '.npy', X_valid)\n",
        "np.save('../dataset/'+ datasplit + 'wise/test/fix_test_' + str(delta) + '.npy', X_test)\n",
        "\n",
        "labels_train.to_csv('../dataset/'+ datasplit +'wise/train/label_train_' + str(delta)+ '.csv', index=False)\n",
        "labels_valid.to_csv('../dataset/'+ datasplit +'wise/val/label_train_' + str(delta) + '.csv', index=False)\n",
        "labels_test.to_csv('../dataset/'+ datasplit + 'wise/test/label_train_' + str(delta) + '.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TbWUpc3UeTq"
      },
      "source": [
        "## model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlF02JZMUil9"
      },
      "outputs": [],
      "source": [
        "## load dataset\n",
        "X_train = np.load('../dataset/'+ datasplit +'wise/train/fix_train_' + str(delta) + '.npy')\n",
        "X_valid = np.load('../dataset/' + datasplit + 'wise/val/fix_valid_' + str(delta) + '.npy')\n",
        "X_test = np.load('../dataset/'+ datasplit + 'wise/test/fix_test_' + str(delta) + '.npy')\n",
        "\n",
        "labels_train = pd.read_csv('../dataset/'+ datasplit +'wise/train/label_train_' + str(delta)+ '.csv')\n",
        "labels_valid = pd.read_csv('../dataset/'+ datasplit +'wise/val/label_train_' + str(delta) + '.csv')\n",
        "labels_test = pd.read_csv('../dataset/'+ datasplit + 'wise/test/label_train_' + str(delta) + '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBT8yscEHH2X"
      },
      "outputs": [],
      "source": [
        "## create dataset\n",
        "pred_variable = 'difficulty'\n",
        "\n",
        "if pred_variable == 'subj':\n",
        "    ## labels as categorical\n",
        "    y_train = labels_train[pred_variable].astype('category').cat.codes\n",
        "    y_valid = labels_valid[pred_variable].astype('category').cat.codes\n",
        "    y_test = labels_test[pred_variable].astype('category').cat.codes\n",
        "\n",
        "else:\n",
        "    ## labels as categorical\n",
        "    y_train = labels_train[pred_variable]\n",
        "    y_valid = labels_valid[pred_variable]\n",
        "    y_test = labels_test[pred_variable]\n",
        "\n",
        "# ## randomize row for training data\n",
        "# from sklearn.utils import shuffle\n",
        "# X_train, labels_train, idx_train = shuffle(X_train, labels_train, idx_train, random_state=23)\n",
        "\n",
        "## data description\n",
        "num_classes = len(pd.unique(y_train)) # labels_train[pred_variable].shape (TTTT,)\n",
        "\n",
        "print(\"##### data description #####\")\n",
        "print(\"# of classes:\\t\",num_classes)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "print(\"input shape is:\\t\",input_shape)\n",
        "\n",
        "N_samples_train = X_train.shape[0]\n",
        "print(\"# of samples for training is:\\t\", N_samples_train)\n",
        "\n",
        "N_samples_valid = X_valid.shape[0]\n",
        "print(\"# of samples for validation is:\\t\", N_samples_valid)\n",
        "\n",
        "N_samples_test = X_test.shape[0]\n",
        "print(\"# of samples for prediction is:\\t\", N_samples_test)\n",
        "\n",
        "N_total = N_samples_train + N_samples_valid + N_samples_test\n",
        "print(\"# of total sampels:\\t\", N_total)\n",
        "\n",
        "## check data imbalances and caculate weights for loss\n",
        "weights = class_weight.compute_class_weight('balanced'\n",
        "        ,np.unique(y_train)\n",
        "        ,y_train)\n",
        "\n",
        "print(\"\\n##### data imbalances #####\")\n",
        "print(y_train.value_counts(normalize=True).sort_index())\n",
        "\n",
        "print(\"\\n##### loss weight #####\")\n",
        "weights = dict(enumerate(weights))\n",
        "print(weights)\n",
        "\n",
        "print(\"\\n##### null acc for test dataset #####\")\n",
        "print(np.max(y_test.value_counts(normalize=True).sort_index()))\n",
        "\n",
        "## one hot encoding\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_valid = np_utils.to_categorical(y_valid, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "# ##### data description #####\n",
        "# # of classes:\t 4\n",
        "# input shape is:\t (41, 4)\n",
        "# # of samples for training is:\t 3216\n",
        "# # of samples for validation is:\t 1109\n",
        "# # of samples for prediction is:\t 1106\n",
        "# # of total sampels:\t 5431\n",
        "\n",
        "# ##### data imbalances #####\n",
        "# 0    0.152674\n",
        "# 1    0.255908\n",
        "# 2    0.317475\n",
        "# 3    0.273943\n",
        "# Name: acc_level, dtype: float64\n",
        "\n",
        "# ##### loss weight #####\n",
        "# {0: 1.6374745417515275, 1: 0.976913730255164, 2: 0.7874632713026445, 3: 0.9125993189557321}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DRXiGEXI1LI"
      },
      "outputs": [],
      "source": [
        "## model specify and compile\n",
        "modeltype = 'rnn'\n",
        "\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "    K.clear_session()\n",
        "\n",
        "if modeltype == 'linear':\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Flatten()(inputs)\n",
        "\n",
        "    # x = Dense(100)(x)\n",
        "    # # x = BatchNormalization()(x)\n",
        "    # x = Activation('relu')(x)\n",
        "\n",
        "    # x = Dense(100)(x)\n",
        "    # # x = BatchNormalization()(x)\n",
        "    # x = Activation('relu')(x)\n",
        "\n",
        "    # x = Dense(100)(x)\n",
        "    # # x = BatchNormalization()(x)\n",
        "    # x = Activation('relu')(x)\n",
        "\n",
        "\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "    model.summary()\n",
        "\n",
        "elif modeltype == 'cnn':\n",
        "\n",
        "    ## 62 bookwise, 64 recordwise for subj_acc_level  (dense 50, 10)\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv1D(40, 3, input_shape= input_shape)) #padding= 'same', #use_bias = False\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv1D(40, 3)) #padding= 'same', #use_bias = False\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv1D(40, 3)) #padding= 'same', #use_bias = False\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    # model.add(Conv1D(20, 3)) #padding= 'same', #use_bias = False\n",
        "    # # model.add(BatchNormalization())\n",
        "    # model.add(Activation('relu'))\n",
        "    # # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(50)) # kernel_regularizer=regularizers.l2(reg)\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(20)) # kernel_regularizer=regularizers.l2(reg)\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "    ## 0422 96% acc\n",
        "    # model = Sequential()\n",
        "\n",
        "    # model.add(Conv1D(64, 3, padding= 'same', input_shape= input_shape)) #use_bias = False\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('relu'))\n",
        "\n",
        "    # model.add(Conv1D(64, 3, padding= 'same'))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('relu'))\n",
        "\n",
        "    # model.add(Conv1D(64, 3))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('relu'))\n",
        "\n",
        "    # # model.add(Conv1D(64, 3))\n",
        "    # # model.add(BatchNormalization())\n",
        "    # # model.add(Activation('relu'))\n",
        "\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dropout(0.5))\n",
        "\n",
        "    # model.add(Dense(20))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('relu'))\n",
        "    # # model.add(Dropout(0.5))\n",
        "\n",
        "    # model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # print(model.summary())\n",
        "\n",
        "\n",
        "elif modeltype == 'rnn':\n",
        "    model = Sequential()\n",
        "\n",
        "    # model.add(BatchNormalization(input_shape=input_shape))\n",
        "    model.add(Bidirectional(LSTM(25, return_sequences = True),input_shape=input_shape))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Bidirectional(CuDNNLSTM(32, return_sequences = True)))\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Bidirectional(LSTM(25)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(50)) # kernel_regularizer=regularizers.l2(reg)\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(20)) # kernel_regularizer=regularizers.l2(reg)\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    print(model.summary())\n",
        "\n",
        "    # model.add(BatchNormalization(input_shape=input_shape))\n",
        "\n",
        "    # #model.add(LSTM(4, input_shape=input_shape, activation='relu'))\n",
        "    # #model.add(LSTM(4, input_shape=input_shape, activation='relu', return_sequences = True))\n",
        "    # #model.add(LSTM(4, activation='relu', return_sequences = True))\n",
        "    # #model.add(LSTM(4, activation='relu', return_sequences = True))\n",
        "    # #model.add(LSTM(4, activation='relu', return_sequences = True))\n",
        "    # #model.add(LSTM(4, activation='relu'))\n",
        "\n",
        "    # # model.add(CuDNNLSTM(4, return_sequences = True))\n",
        "    # # model.add(CuDNNLSTM(4, return_sequences = True))\n",
        "    # # #model.add(LSTM(4, activation='relu', return_sequences = True))\n",
        "    # # model.add(CuDNNLSTM(4))\n",
        "\n",
        "    # # model.add(CuDNNGRU(4, return_sequences = True))\n",
        "    # # model.add(CuDNNGRU(4, return_sequences = True))\n",
        "    # # #model.add(LSTM(4, activation='relu', return_sequences = True))\n",
        "    # # model.add(CuDNNGRU(4))\n",
        "\n",
        "    # #model.add(Bidirectional(CuDNNLSTM(32)))\n",
        "    # model.add(Bidirectional(CuDNNLSTM(32, return_sequences = True)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    # model.add(Bidirectional(CuDNNLSTM(32, return_sequences = True)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    # # #model.add(LSTM(4, activation='relu', return_sequences = True))\n",
        "    # model.add(Bidirectional(CuDNNLSTM(32)))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Dropout(0.1))\n",
        "\n",
        "    # #model.add(Dropout(0.2))\n",
        "\n",
        "    # # model.add(Dense(10))\n",
        "    # # model.add(BatchNormalization())\n",
        "    # # model.add(Activation('relu'))\n",
        "\n",
        "    # model.add(Dense(num_classes, activation='softmax'))\n",
        "    # model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycn_g_aEK1j4"
      },
      "outputs": [],
      "source": [
        "# BATCH_SIZE = 100\n",
        "# EPOCHS = 1000\n",
        "\n",
        "#learning_rate = 0.0001\n",
        "#decay_rate= 1e-04\n",
        "#optimizer = RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=decay_rate)\n",
        "#optimizer = SGD(lr=learning_rate, decay=decay_rate, momentum=0.9, nesterov=True)\n",
        "#optimizer = Adagrad(lr=learning_rate, epsilon=None, decay=decay_rate)\n",
        "#optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n",
        "#optimizer = Adadelta()\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n",
        "model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "## checkpoint path\n",
        "date_time = datetime.now()\n",
        "d = date_time.strftime(\"%m%d%H%M\")\n",
        "print(d)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(\n",
        "        filepath= '../checkpoint/' + str(d) +'-{epoch:02d}-{val_acc:.2f}.h5',\n",
        "        monitor='val_acc', save_best_only=True),\n",
        "    EarlyStopping(monitor='val_loss', patience=50)\n",
        "]\n",
        "\n",
        "hist = model.fit(X_train, y_train, batch_size=BATCH_SIZE,  epochs =EPOCHS,\n",
        "                 class_weight = weights, verbose=2, callbacks=callbacks_list,\n",
        "                 validation_data= (X_valid, y_valid), shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvXoO0uan4qn"
      },
      "outputs": [],
      "source": [
        "## visualizing losses and accuracy\n",
        "\n",
        "## figures path\n",
        "fig_loss = path_result +'/figure/rnn0430_norm_loss_to'  + str(datasize) + '_w'+ str(delta) + '.png'\n",
        "fig_acc =  path_result +'/figure/rnn0430_norm_acc_to'  + str(datasize) + '_w'+ str(delta) + '.png'\n",
        "\n",
        "\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['acc']\n",
        "val_acc = hist.history['val_acc']\n",
        "xc = range(201) #EPOCHS run or see whether early stoppings\n",
        "\n",
        "# train_loss =  hist.history['loss'] + hist2.history['loss']\n",
        "# val_loss = hist.history['val_loss'] + hist2.history['val_loss']\n",
        "# train_acc = hist.history['acc'] + hist2.history['acc']\n",
        "# val_acc = hist.history['val_acc'] + hist2.history['val_acc']\n",
        "# xc = range(2*EPOCHS) #EPOCHS run or see whether early stoppings\n",
        "\n",
        "\n",
        "plt.figure(1, figsize=(7, 5), facecolor=\"white\")\n",
        "plt.plot(xc, train_loss)\n",
        "plt.plot(xc, val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train', 'val'])\n",
        "# print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "plt.savefig(fig_loss)\n",
        "\n",
        "plt.figure(2, figsize=(7, 5),facecolor=\"white\")\n",
        "plt.plot(xc, train_acc)\n",
        "plt.plot(xc, val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train', 'val']) #, loc=4\n",
        "# print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "plt.savefig(fig_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MjNfTxXnqHK"
      },
      "source": [
        "## evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbGlMcrgoHOi"
      },
      "outputs": [],
      "source": [
        "datasplit = 'subject'\n",
        "\n",
        "## load dataset\n",
        "X_train = np.load('../dataset/'+ datasplit +'wise/train/fix_train_' + str(delta) + '.npy')\n",
        "X_valid = np.load('../dataset/' + datasplit + 'wise/val/fix_valid_' + str(delta) + '.npy')\n",
        "X_test = np.load('../dataset/'+ datasplit + 'wise/test/fix_test_' + str(delta) + '.npy')\n",
        "\n",
        "labels_train = pd.read_csv('../dataset/'+ datasplit +'wise/train/label_train_' + str(delta)+ '.csv')\n",
        "labels_valid = pd.read_csv('../dataset/'+ datasplit +'wise/val/label_train_' + str(delta) + '.csv')\n",
        "labels_test = pd.read_csv('../dataset/'+ datasplit + 'wise/test/label_train_' + str(delta) + '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdf_DMWZq8vM"
      },
      "outputs": [],
      "source": [
        "pred_variable = 'native'\n",
        "\n",
        "## create dataset\n",
        "if pred_variable == 'subj':\n",
        "    ## labels as categorical\n",
        "    y_train = labels_train[pred_variable].astype('category').cat.codes\n",
        "    y_valid = labels_valid[pred_variable].astype('category').cat.codes\n",
        "    y_test = labels_test[pred_variable].astype('category').cat.codes\n",
        "\n",
        "else:\n",
        "    ## labels as categorical\n",
        "    y_train = labels_train[pred_variable]\n",
        "    y_valid = labels_valid[pred_variable]\n",
        "    y_test = labels_test[pred_variable]\n",
        "\n",
        "# ## randomize row for training data\n",
        "# from sklearn.utils import shuffle\n",
        "# X_train, labels_train, idx_train = shuffle(X_train, labels_train, idx_train, random_state=23)\n",
        "\n",
        "## data description\n",
        "num_classes = len(pd.unique(y_train)) # labels_train[pred_variable].shape (TTTT,)\n",
        "\n",
        "print(\"##### data description #####\")\n",
        "print(\"# of classes:\\t\",num_classes)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "print(\"input shape is:\\t\",input_shape)\n",
        "\n",
        "N_samples_train = X_train.shape[0]\n",
        "print(\"# of samples for training is:\\t\", N_samples_train)\n",
        "\n",
        "N_samples_valid = X_valid.shape[0]\n",
        "print(\"# of samples for validation is:\\t\", N_samples_valid)\n",
        "\n",
        "N_samples_test = X_test.shape[0]\n",
        "print(\"# of samples for prediction is:\\t\", N_samples_test)\n",
        "\n",
        "N_total = N_samples_train + N_samples_valid + N_samples_test\n",
        "print(\"# of total sampels:\\t\", N_total)\n",
        "\n",
        "## check data imbalances and caculate weights for loss\n",
        "weights = class_weight.compute_class_weight('balanced'\n",
        "        ,np.unique(y_train)\n",
        "        ,y_train)\n",
        "\n",
        "print(\"\\n##### data imbalances #####\")\n",
        "print(y_train.value_counts(normalize=True).sort_index())\n",
        "\n",
        "print(\"\\n##### loss weight #####\")\n",
        "weights = dict(enumerate(weights))\n",
        "print(weights)\n",
        "\n",
        "print(\"\\n##### null acc for test dataset #####\")\n",
        "print(np.max(y_test.value_counts(normalize=True).sort_index()))\n",
        "\n",
        "## one hot encoding\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_valid = np_utils.to_categorical(y_valid, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmOOxtDMuajH"
      },
      "outputs": [],
      "source": [
        "## load model\n",
        "from keras.models import load_model\n",
        "\n",
        "modeltype = 'rnn'\n",
        "\n",
        "file_model = '../savedmodel/'+ pred_variable + '/' + datasplit+ 'wise/' + modeltype + '.h5'\n",
        "loaded_model = load_model(file_model)\n",
        "print(\"Loaded model from disk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-TMdgh1LQtc"
      },
      "outputs": [],
      "source": [
        "## Evaluating the model\n",
        "print(\"predicted variable:\", pred_variable, '\\n')\n",
        "\n",
        "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Printing the confusion matrix\n",
        "\n",
        "Y_pred = loaded_model.predict(X_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "#y_pred = model.predict_classes(X_test)\n",
        "# target_names = ['F', 'M']\n",
        "# target_names = ['sleepy', 'awake']\n",
        "#target_names = ['hard', 'easy']\n",
        "\n",
        "target_names = ['level {}'.format(i) for i in range(num_classes)]\n",
        "\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
        "print(\"confusion matrix: \\n\", confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
        "\n",
        "# cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "# bas = (0.5*cm[0,0])/(cm[0,0]+cm[0,1]) + (0.5*cm[1,1])/(cm[1,0]+cm[1,1])\n",
        "# print (\"Balanced acc score:\",bas )\n",
        "# print (\"Balanced error rate:\",1 - bas)\n",
        "\n",
        "print(\"Balanced acc score:\", balanced_accuracy_score(np.argmax(y_test, axis=1), y_pred))\n",
        "print(\"Balanced error rate:\", 1- balanced_accuracy_score(np.argmax(y_test, axis=1), y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPs20taSL8me"
      },
      "outputs": [],
      "source": [
        "## confusion matrix\n",
        "sns.set_style(\"white\")\n",
        "cfmtrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "\n",
        "df_cm = pd.DataFrame(cfmtrix, index = [i for i in target_names],\n",
        "                  columns = [i for i in target_names])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True,fmt=\"d\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}